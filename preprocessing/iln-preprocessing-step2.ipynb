{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## todo list:\n",
    "- Fix the removing of waypoints (should be fixed?)\n",
    "- Create a dataset of waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from glob import glob\n",
    "from joblib import Parallel,delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils/\")\n",
    "from iln_io_f import read_data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob(\"../data/train/*.parquet\")\n",
    "test_files  = glob(\"../data/test/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/bssid_by_site.yml\", \"r\") as file:\n",
    "    bssid_by_site = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "all_sites = list(bssid_by_site.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Creating dataset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../data/ds1/train\n",
    "!mkdir -p ../data/ds1/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum percentaje of observation to be considered as top seen bssid\n",
    "MIN_PERC = 0.05\n",
    "\n",
    "# params to configure near bssids\n",
    "MAX_TIME_DIFF = 10000\n",
    "MIN_RSSI = -70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seq_nbr(ts_serie):\n",
    "    \"\"\"\n",
    "    Function to calculate the sequence number: for each path, the \n",
    "    sequence number corresponds to the ordered index of each \n",
    "    waypoint observation\n",
    "    \"\"\"\n",
    "    mapping = {v:i for i,v in enumerate(np.sort(ts_serie.unique()))}\n",
    "    return ts_serie.map(mapping)\n",
    "\n",
    "def compute_near_bssids(df, all_bssids, max_time_diff=MAX_TIME_DIFF, min_rssi=MIN_RSSI):\n",
    "    \"\"\"\n",
    "    Function to calculate near bssids for each path,timestamp\n",
    "    \"\"\"\n",
    "    near_bssids = (df.query(\"wifi_time_diff <= @max_time_diff | index == 0\")[all_bssids] >= min_rssi).any()\n",
    "    near_bssids = near_bssids[near_bssids]\n",
    "    return near_bssids.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 208/626 [00:25<00:55,  7.59it/s]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "100%|██████████| 626/626 [01:19<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# reads test files in parallel\n",
    "with Parallel(n_jobs=8) as parallel:\n",
    "    delayed_func = delayed(pd.read_parquet)\n",
    "    test_dataframes = parallel(delayed_func(f) for f in tqdm(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [03:46<00:00,  9.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# finds the bssids that must be kept, based on test observations\n",
    "def compute_bssids_to_keep(test_dataframes, site_id):\n",
    "    all_bssids = bssid_by_site[site_id]\n",
    "    df = pd.concat(filter(lambda x: x.site.unique()[0] == site_id, test_dataframes), axis=0, ignore_index=True)\n",
    "    df[\"wifi_time_diff\"] = np.abs(df.wifi_time_delta)\n",
    "    \n",
    "    keep_bssids = np.concatenate(df.groupby([\"path\",\"timestamp\"]).apply(compute_near_bssids, all_bssids).values)\n",
    "    keep_bssids = list(set(keep_bssids))\n",
    "    return (site_id, keep_bssids)\n",
    "\n",
    "with Parallel(n_jobs=8) as parallel:\n",
    "    delayed_func = delayed(compute_bssids_to_keep)\n",
    "    keep_bssids_by_site = parallel(delayed_func(test_dataframes, site) for site in tqdm(all_sites))\n",
    "keep_bssids_by_site = {k:v for k,v in keep_bssids_by_site}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a0546857ecc773753327266 1809\n",
      "5c3c44b80379370013e0fd2b 511\n",
      "5d27075f03f801723c2e360f 487\n",
      "5d27096c03f801723c31e5e0 667\n",
      "5d27097f03f801723c320d97 591\n",
      "5d27099f03f801723c32511d 283\n",
      "5d2709a003f801723c3251bf 618\n",
      "5d2709b303f801723c327472 893\n",
      "5d2709bb03f801723c32852c 1215\n",
      "5d2709c303f801723c3299ee 3273\n",
      "5d2709d403f801723c32bd39 947\n",
      "5d2709e003f801723c32d896 624\n",
      "5da138274db8ce0c98bbd3d2 162\n",
      "5da1382d4db8ce0c98bbe92e 1316\n",
      "5da138314db8ce0c98bbf3a0 760\n",
      "5da138364db8ce0c98bc00f1 271\n",
      "5da1383b4db8ce0c98bc11ab 638\n",
      "5da138754db8ce0c98bca82f 550\n",
      "5da138764db8ce0c98bcaa46 863\n",
      "5da1389e4db8ce0c98bd0547 212\n",
      "5da138b74db8ce0c98bd4774 1415\n",
      "5da958dd46f8266d0737457b 2228\n",
      "5dbc1d84c1eb61796cf7c010 2733\n",
      "5dc8cea7659e181adb076a3f 1237\n"
     ]
    }
   ],
   "source": [
    "for k,v in keep_bssids_by_site.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing site 1/24: 5da1389e4db8ce0c98bd0547\n",
      "Min number of observations: 315 from 6312\n",
      "Selected 265 bssids from 1021 (25.95%)\n",
      "\n",
      "Processing site 2/24: 5d27099f03f801723c32511d\n",
      "Min number of observations: 212 from 4251\n",
      "Selected 567 bssids from 925 (61.30%)\n",
      "\n",
      "Processing site 3/24: 5d2709b303f801723c327472\n",
      "Min number of observations: 767 from 15358\n",
      "Selected 991 bssids from 1913 (51.80%)\n",
      "\n",
      "Processing site 4/24: 5dc8cea7659e181adb076a3f\n",
      "Min number of observations: 782 from 15655\n",
      "Selected 1396 bssids from 4864 (28.70%)\n",
      "rows with no signal removed in dataset: 326\n",
      "\n",
      "Processing site 5/24: 5d2709c303f801723c3299ee\n",
      "Min number of observations: 504 from 10083\n",
      "Selected 3852 bssids from 5831 (66.06%)\n",
      "\n",
      "Processing site 6/24: 5d2709d403f801723c32bd39\n",
      "Min number of observations: 501 from 10027\n",
      "Selected 1016 bssids from 2139 (47.50%)\n",
      "\n",
      "Processing site 7/24: 5d27097f03f801723c320d97\n",
      "Min number of observations: 525 from 10507\n",
      "Selected 855 bssids from 2490 (34.34%)\n",
      "rows with no signal removed in dataset: 2704\n",
      "waypoints prev: 3795 waypoints now: 3731 diff: 64\n",
      "\n",
      "Processing site 8/24: 5d27075f03f801723c2e360f\n",
      "Min number of observations: 1183 from 23666\n",
      "Selected 1886 bssids from 7029 (26.83%)\n",
      "rows with no signal removed in dataset: 6280\n",
      "waypoints prev: 7286 waypoints now: 7259 diff: 27\n",
      "\n",
      "Processing site 9/24: 5d27096c03f801723c31e5e0\n",
      "Min number of observations: 455 from 9100\n",
      "Selected 672 bssids from 4964 (13.54%)\n",
      "\n",
      "Processing site 10/24: 5c3c44b80379370013e0fd2b\n",
      "Min number of observations: 486 from 9737\n",
      "Selected 1495 bssids from 3063 (48.81%)\n",
      "\n",
      "Processing site 11/24: 5da138364db8ce0c98bc00f1\n",
      "Min number of observations: 138 from 2767\n",
      "Selected 398 bssids from 822 (48.42%)\n",
      "\n",
      "Processing site 12/24: 5da958dd46f8266d0737457b\n",
      "Min number of observations: 757 from 15148\n",
      "Selected 2325 bssids from 3499 (66.45%)\n",
      "\n",
      "Processing site 13/24: 5a0546857ecc773753327266\n",
      "Min number of observations: 464 from 9296\n",
      "Selected 2122 bssids from 3397 (62.47%)\n",
      "\n",
      "Processing site 14/24: 5da138314db8ce0c98bbf3a0\n",
      "Min number of observations: 450 from 9012\n",
      "Selected 818 bssids from 1212 (67.49%)\n",
      "\n",
      "Processing site 15/24: 5d2709bb03f801723c32852c\n",
      "Min number of observations: 860 from 17203\n",
      "Selected 1348 bssids from 2452 (54.98%)\n",
      "rows with no signal removed in dataset: 4576\n",
      "waypoints prev: 3613 waypoints now: 3603 diff: 10\n",
      "\n",
      "Processing site 16/24: 5da1383b4db8ce0c98bc11ab\n",
      "Min number of observations: 659 from 13196\n",
      "Selected 870 bssids from 1525 (57.05%)\n",
      "\n",
      "Processing site 17/24: 5da1382d4db8ce0c98bbe92e\n",
      "Min number of observations: 449 from 8999\n",
      "Selected 1646 bssids from 2862 (57.51%)\n",
      "\n",
      "Processing site 18/24: 5d2709a003f801723c3251bf\n",
      "Min number of observations: 197 from 3940\n",
      "Selected 735 bssids from 1252 (58.71%)\n",
      "\n",
      "Processing site 19/24: 5da138754db8ce0c98bca82f\n",
      "Min number of observations: 359 from 7188\n",
      "Selected 629 bssids from 1627 (38.66%)\n",
      "\n",
      "Processing site 20/24: 5da138b74db8ce0c98bd4774\n",
      "Min number of observations: 869 from 17382\n",
      "Selected 1697 bssids from 3535 (48.01%)\n",
      "rows with no signal removed in dataset: 2054\n",
      "waypoints prev: 6111 waypoints now: 6097 diff: 14\n",
      "\n",
      "Processing site 21/24: 5d2709e003f801723c32d896\n",
      "Min number of observations: 552 from 11042\n",
      "Selected 677 bssids from 1309 (51.72%)\n",
      "\n",
      "Processing site 22/24: 5da138764db8ce0c98bcaa46\n",
      "Min number of observations: 471 from 9420\n",
      "Selected 910 bssids from 1888 (48.20%)\n",
      "\n",
      "Processing site 23/24: 5dbc1d84c1eb61796cf7c010\n",
      "Min number of observations: 808 from 16174\n",
      "Selected 2807 bssids from 4519 (62.12%)\n",
      "\n",
      "Processing site 24/24: 5da138274db8ce0c98bbd3d2\n",
      "Min number of observations: 133 from 2662\n",
      "Selected 251 bssids from 490 (51.22%)\n",
      "CPU times: user 3min 31s, sys: 2min 17s, total: 5min 48s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "top_seen_bssids_by_site = dict()\n",
    "\n",
    "for i,file in enumerate(train_files):\n",
    "    \n",
    "    site_id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(f\"\\nProcessing site {i+1}/{len(train_files)}: {site_id}\")\n",
    "\n",
    "    df = pd.read_parquet(file)\n",
    "    df[\"site\"] = site_id\n",
    "    df[\"wifi_time_diff\"] = np.abs(df.wifi_time_delta)\n",
    "    df[\"seq_nbr\"] = df.groupby(\"path\")[\"timestamp\"].apply(compute_seq_nbr)\n",
    "    \n",
    "    all_bssids = bssid_by_site[site_id]\n",
    "    keep_bssids = keep_bssids_by_site[site_id]\n",
    "\n",
    "    # selects the top seen bssids for the site\n",
    "    n_wifi_obs = len(df.loc[:,[\"path\",\"timestamp_wifi\"]].drop_duplicates())\n",
    "    min_obs = int(n_wifi_obs*MIN_PERC)\n",
    "    count_by_bssid = (df.query(\"seq_nbr == 0\")[all_bssids] > -999).sum(axis=0)\n",
    "    top_seen_bssids = count_by_bssid[count_by_bssid > min_obs].index.tolist()\n",
    "    top_seen_bssids = list(set(top_seen_bssids) | set(keep_bssids))\n",
    "    top_seen_bssids_by_site[site_id] = top_seen_bssids\n",
    "    bssids_to_remove = list(set(all_bssids) - set(top_seen_bssids))\n",
    "    print(\"Min number of observations:\", min_obs, \"from\", n_wifi_obs)\n",
    "    \n",
    "    n_top = len(top_seen_bssids)\n",
    "    n_all = len(all_bssids)\n",
    "    print(f\"Selected {n_top} bssids from {n_all} ({100*n_top/n_all:.2f}%)\")\n",
    "    \n",
    "    df.drop(bssids_to_remove, axis=1, inplace=True)\n",
    "    n_waypoints = len(df.loc[:,[\"path\",\"timestamp\"]].drop_duplicates())\n",
    "    \n",
    "    # sanity check\n",
    "    if (df[top_seen_bssids] == -999).all(axis=0).any():\n",
    "        cols_to_drop = df[top_seen_bssids].columns[(df[top_seen_bssids] == -999).all(axis=0)]\n",
    "        df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "        print(\"columns with no signal removed in dataset:\", len(cols_to_drop))\n",
    "    if (df[top_seen_bssids] == -999).all(axis=1).any():\n",
    "        idx_to_drop = df.index[(df[top_seen_bssids] == -999).all(axis=1)]\n",
    "        df.drop(idx_to_drop, inplace=True)\n",
    "        print(\"rows with no signal removed in dataset:\", len(idx_to_drop))\n",
    "        \n",
    "        # verifies if there are no missing waypoints\n",
    "        #assert n_waypoints == len(df.loc[:,[\"path\",\"timestamp\"]].drop_duplicates()), \"Missing waypoints\"\n",
    "        n_waypoints_new = len(df.loc[:,[\"path\",\"timestamp\"]].drop_duplicates())\n",
    "        if n_waypoints_new != n_waypoints:\n",
    "            print(\"waypoints prev:\", n_waypoints, \"waypoints now:\", n_waypoints_new, \"diff:\", n_waypoints-n_waypoints_new)\n",
    "        \n",
    "    df.to_parquet(f\"../data/ds1/train/{site_id}.parquet\", index=False)\n",
    "    del df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 160/626 [00:20<00:54,  8.59it/s]/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "100%|██████████| 626/626 [01:22<00:00,  7.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# reads test files in parallel\n",
    "with Parallel(n_jobs=8) as parallel:\n",
    "    delayed_func = delayed(pd.read_parquet)\n",
    "    test_dataframes = parallel(delayed_func(f) for f in tqdm(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing site 1/24: 5a0546857ecc773753327266\n",
      "columns with no signal in dataset: 103 from 2122\n",
      "\n",
      "Processing site 2/24: 5c3c44b80379370013e0fd2b\n",
      "columns with no signal in dataset: 601 from 1495\n",
      "\n",
      "Processing site 3/24: 5d27075f03f801723c2e360f\n",
      "columns with no signal in dataset: 895 from 1886\n",
      "\n",
      "Processing site 4/24: 5d27096c03f801723c31e5e0\n",
      "\n",
      "Processing site 5/24: 5d27097f03f801723c320d97\n",
      "columns with no signal in dataset: 48 from 855\n",
      "\n",
      "Processing site 6/24: 5d27099f03f801723c32511d\n",
      "columns with no signal in dataset: 100 from 567\n",
      "\n",
      "Processing site 7/24: 5d2709a003f801723c3251bf\n",
      "columns with no signal in dataset: 26 from 735\n",
      "\n",
      "Processing site 8/24: 5d2709b303f801723c327472\n",
      "columns with no signal in dataset: 34 from 991\n",
      "\n",
      "Processing site 9/24: 5d2709bb03f801723c32852c\n",
      "columns with no signal in dataset: 57 from 1348\n",
      "\n",
      "Processing site 10/24: 5d2709c303f801723c3299ee\n",
      "columns with no signal in dataset: 124 from 3852\n",
      "\n",
      "Processing site 11/24: 5d2709d403f801723c32bd39\n",
      "\n",
      "Processing site 12/24: 5d2709e003f801723c32d896\n",
      "columns with no signal in dataset: 5 from 677\n",
      "\n",
      "Processing site 13/24: 5da138274db8ce0c98bbd3d2\n",
      "columns with no signal in dataset: 12 from 251\n",
      "\n",
      "Processing site 14/24: 5da1382d4db8ce0c98bbe92e\n",
      "columns with no signal in dataset: 151 from 1646\n",
      "\n",
      "Processing site 15/24: 5da138314db8ce0c98bbf3a0\n",
      "columns with no signal in dataset: 5 from 818\n",
      "\n",
      "Processing site 16/24: 5da138364db8ce0c98bc00f1\n",
      "columns with no signal in dataset: 43 from 398\n",
      "\n",
      "Processing site 17/24: 5da1383b4db8ce0c98bc11ab\n",
      "columns with no signal in dataset: 34 from 870\n",
      "\n",
      "Processing site 18/24: 5da138754db8ce0c98bca82f\n",
      "columns with no signal in dataset: 2 from 629\n",
      "\n",
      "Processing site 19/24: 5da138764db8ce0c98bcaa46\n",
      "columns with no signal in dataset: 4 from 910\n",
      "\n",
      "Processing site 20/24: 5da1389e4db8ce0c98bd0547\n",
      "columns with no signal in dataset: 6 from 265\n",
      "\n",
      "Processing site 21/24: 5da138b74db8ce0c98bd4774\n",
      "columns with no signal in dataset: 20 from 1697\n",
      "\n",
      "Processing site 22/24: 5da958dd46f8266d0737457b\n",
      "\n",
      "Processing site 23/24: 5dbc1d84c1eb61796cf7c010\n",
      "\n",
      "Processing site 24/24: 5dc8cea7659e181adb076a3f\n",
      "columns with no signal in dataset: 1 from 1396\n"
     ]
    }
   ],
   "source": [
    "for i,site_id in enumerate(all_sites):\n",
    "    \n",
    "    print(f\"\\nProcessing site {i+1}/{len(all_sites)}: {site_id}\")\n",
    "    \n",
    "    all_bssids = bssid_by_site[site_id]\n",
    "    top_seen_bssids = top_seen_bssids_by_site[site_id] \n",
    "    bssids_to_remove = list(set(all_bssids)-set(top_seen_bssids))\n",
    "    \n",
    "    df = pd.concat(filter(lambda x: x.site.unique()[0] == site_id, test_dataframes), axis=0, ignore_index=True)\n",
    "    df[\"site\"] = site_id\n",
    "    df[\"wifi_time_diff\"] = np.abs(df.wifi_time_delta)\n",
    "    df[\"seq_nbr\"] = df.groupby(\"path\")[\"timestamp\"].apply(compute_seq_nbr)\n",
    "    df.drop(bssids_to_remove, axis=1, inplace=True)\n",
    "    n_pred_points = df.site_path_timestamp.nunique()\n",
    "    \n",
    "    # sanity check\n",
    "    if (df[top_seen_bssids] == -999).all(axis=0).any():\n",
    "        cols_to_drop = df[top_seen_bssids].columns[(df[top_seen_bssids] == -999).all(axis=0)]\n",
    "        # columns are not dropped beacuse can be useful for fitting the training set\n",
    "        print(f\"columns with no signal in dataset: {len(cols_to_drop)} from {len(top_seen_bssids)}\")\n",
    "        \n",
    "    if (df[top_seen_bssids] == -999).all(axis=1).any():\n",
    "        idx_to_drop = df.index[(df[top_seen_bssids] == -999).all(axis=1)]\n",
    "        df.drop(idx_to_drop, inplace=True)\n",
    "        print(f\"rows with no signal removed in dataset: {len(idx_to_drop)} from {len(df)}\")\n",
    "        \n",
    "        # verifies if there are no missing prediction points\n",
    "        assert n_pred_points == df.site_path_timestamp.nunique(), \"Missing prediction points\"\n",
    "    \n",
    "    df.to_parquet(f\"../data/ds1/test/{site_id}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file test.zip\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.10)\n",
      "100%|██████████████████████████████████████| 33.3M/33.3M [00:01<00:00, 20.2MB/s]\n",
      "Upload successful: test.zip (33MB)\n",
      "Starting upload for file train.zip\n",
      "100%|████████████████████████████████████████| 164M/164M [00:02<00:00, 58.5MB/s]\n",
      "Upload successful: train.zip (164MB)\n",
      "Dataset version is being created. Please check progress at https://www.kaggle.com/mavillan/iln-dataset1\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets version -r zip -p ../data/ds1 -m \"Adds bssids near to prediction points in test dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## dataset-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
